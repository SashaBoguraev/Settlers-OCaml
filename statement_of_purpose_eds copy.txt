
How do humans learn?  State-of-the-art learning algorithms and generative artificial intelligence do not answer this, but they have emerged as powerful and interpretable tools for approximating and studying human learning and cognition.  I am particularly interested in two multidisciplinary questions:

* How to use modern algorithms to model and understand our own cognitive processes?
* How to use cognitively informed principles to build more powerful, reliable and efficient artificial intelligence systems? 

My research to date uses multi-agent reinforcement learning (MARL) to study the communicative advantages conferred by certain traits counterintuitively prevalent in human language.  My goal -- gaining insights about learning and generalization cognitive processes -- underpins the PhD I intend to pursue, itself leveraging these for building better AI systems.

My research activities are not confined to MARL alone, and following the trajectory they defined, I learned to be a strongly independent researcher.  My intent at Cornell was to study Biology and Statistics with an eye on a career in Computational Biology.  To this end, I joined the Van Wijk lab studying protein systems in thale cress, a model plant organism.  Being the only person in this lab with a computational bent, I worked directly with Dr. van Wijk on three largely independent computational projects. 

Firstly, I manually compared mass spectrometry data to canonical gene data in IGV for a class of small peptides which were commonly identifiable, but otherwise little kknown.  This analysis was part of a larger effort to build a comprehensive database of all thale cress peptides, and the work was published as a peer-reviewed joint paper, for which I was co-author.

Next, I helped integrate a data processing pipeline into the lab’s workflow, allowing for processing of co-expression data previously collected for the CLP family of chloroplast proteins, thought to play an important role in protein homeostasis.  This project gave me my second joint publication.

My third project (summer of 2022) developed a dynamical systems model of the CLP system in Mathematica and MATLAB.  Implementing closed-form solutions and visualizing the system dynamics, I uncovered multiple fixed points and bifurcations in state space which could inform future experimental research into CLP.

While I enjoyed these projects, my interests clearly lay more in the computational aspects of research than with the life sciences.  This motivated a pivotal decision, after my sophomore year to focus my studies on Computer Science.  I had to convert some pass-fail core CS courses already taken, such as Data Structures, into somewhat unsavory grades, but it gave me a much more appropriate basis to explore my interests.                   

Another pivotal moment in my Cornell research career was gaining acceptance to the College Scholar Program -- an interdisciplinary major comprising two years of independent research culminating in a senior year honors thesis.  I was interested in deploying computational modeling to study language cognitive processes.  A thesis committee (Dr. Veit Elser, Cornell Physics; Dr. Morten Christensen, Cornell Psychology, and Dr. Eugene Vinitsky, NYU Tandon Transportation Engineering) provides guidance and mentorshp.

My thesis asks, for the first time: why does human language consist of a set of idiolects, instead of comprising a single uniform entity?  Why is language considered a fixed system, whereas individuals have their each unique understanding?  Messages passed during communication are not exactly identical at transmitter and receiver ends (even if what is received is “good enough” approximation).  This is counter-intuitive -- universal understanding should be more efficient -- and deserves further studies.

MARL is a burgeoning technology capable of driveing emergent communication, and it can function as a tool building populations of agents with idiolectical and universal languages.  Their performance then can be predicted and studied; eg we expect universal populations to perform better in fixed environment tasks, whereas the populations of idiolects would perform better in tasks requiring generalization.  This would say something about our communication systems likely evolved to allow ease of communication in our non-fixed world.

During my junior summer, I interned at NASA’s Jet Propulsion Laboratories (JPL).  JPL was having issues with  missions running into last minute problems. These issues, potentially discoverable by internal auditing, led to massive, unnecessary expenses. To mitigate these costs, JPL wanted to explore large language models for intelligent auditing. My research utilized Facebook’s LLaMa2 and Vicuña models in a retrieval-augmented generation system, with a fine-tuned retrieval mechanism, to develop a proof-of-concept, intelligent JPL rules assistant.  My work was deemed of high quality and strategic importance, and I had the honor of presenting to the deputy director of JPL and all chief engineers.  An additional benefit from being in JPL's administrative branch was that since no one there had NLP background, the project exercised and reinforced the independent research skill previously acquired throughout my research career.

Outside of research, I greatly enjoy teaching.  I am currently a TA for CS 4740: Natural Language Processing.  As a practicum course, students have four month long projects. With four other TAs, I performed extensive testing of the first project, building a Hidden Markov Model and Maximum Entropy Markov model for Named Entity Recognition tagging.  Further, with the same team of TAs, I created the third homework: using LSTMs in an encoder-decoder architecture and augmented natural language to perform Semantic Role Labeling.  These activities, along with office hours and other course responsibilities, demand time and effort.  However, seeing students learning firsthand is extremely rewarding.  It is those moments which make me all the more excited to spend my future in academia. 
 
